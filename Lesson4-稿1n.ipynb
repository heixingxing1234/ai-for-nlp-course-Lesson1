{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptn1 = '<.+?>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptn2 = \"[“”，。「」（）《》、\\'·\\\"\\-]\"# remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r'F:\\NLP训练\\zhwiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filelist(dir):\n",
    "\n",
    "    Filelist = []\n",
    "\n",
    "    for home, dirs, files in os.walk(path):\n",
    "\n",
    "        for filename in files:\n",
    "\n",
    "            # 文件名列表，包含完整路径\n",
    "\n",
    "            Filelist.append(os.path.join(home, filename))\n",
    "\n",
    "            # # 文件名列表，只包含文件名\n",
    "\n",
    "            # Filelist.append( filename)\n",
    "\n",
    "    return Filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filelist = get_filelist(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hanziconv in f:\\anaconda\\anaconda\\lib\\site-packages (0.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install hanziconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanziconv import HanziConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "outfile = open('new.txt', 'w',encoding='utf-8')\n",
    "Filelist = get_filelist(dir)\n",
    "for file in  Filelist :\n",
    "    with open(file,encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = re.sub(ptn1, '', line)\n",
    "            line = re.sub(ptn2, '', line)\n",
    "            line = HanziConv.toSimplified(line)\n",
    "            outfile.write(line)\n",
    "        print(\"Done!\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"F:/div/pick/1.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        with open(line[0], \"a\") as f:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filelist = get_filelist(dir)\n",
    "for file in  Filelist :\n",
    "    with open(file,encoding='utf-8') as f:\n",
    "        with open((file + str('k')), 'w',encoding='utf-8') as outfile:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = re.sub(ptn1, '', line)\n",
    "                line = re.sub(ptn2, '', line)\n",
    "                line = HanziConv.toSimplified(line)\n",
    "                outfile.write(line)\n",
    "                #print(\"Done!\")\n",
    "        #outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 =r'F:\\NLP训练\\zhwiki1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filelist1(dir):\n",
    "\n",
    "    Filelist = []\n",
    "\n",
    "    for home, dirs, files in os.walk(path1):\n",
    "\n",
    "        for filename in files:\n",
    "\n",
    "            # 文件名列表，包含完整路径\n",
    "\n",
    "            Filelist.append(os.path.join(home, filename))\n",
    "\n",
    "            # # 文件名列表，只包含文件名\n",
    "\n",
    "            # Filelist.append( filename)\n",
    "\n",
    "    return Filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filelist2(dir):\n",
    "\n",
    "    Filelist = []\n",
    "\n",
    "    for home, dirs, files in os.walk(path1):\n",
    "\n",
    "        for dirname in dirs:\n",
    "\n",
    "            # 文件名列表，包含完整路径\n",
    "\n",
    "            Filelist.append(os.path.join(home, dirname))\n",
    "\n",
    "            # # 文件名列表，只包含文件名\n",
    "\n",
    "            # Filelist.append( filename)\n",
    "\n",
    "    return Filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filelist2 = get_filelist2(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AA', 'AB', 'AC', 'AD', 'AE', 'AF', 'AG', 'AH', 'AI', 'AJ', 'AK', 'AL', 'AM']\n"
     ]
    }
   ],
   "source": [
    "dirss = []\n",
    "for dirname in Filelist2:\n",
    "    d = dirname.split('\\\\')[4]\n",
    "    dirss.append(d) \n",
    "print(dirss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 =r'F:\\NLP训练'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filelist3 = []\n",
    "file_name1 = path2 +str('\\\\')+ str('zhwiki2')\n",
    "os.makedirs(file_name1) #创建文件夹\n",
    "for dirs in Filelist2:\n",
    "    file_name = file_name1 + str('\\\\') +  dirs.split('\\\\')[4]    \n",
    "    Filelist3.append(file_name)\n",
    "    os.makedirs(file_name) #创建文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptn1 = '<.+?>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptn2 = \"[“”，。「」（）《》、\\'·\\\"\\-]\"# remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanziconv import HanziConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filelist1 = get_filelist1(dir) \n",
    "for file in  Filelist1:\n",
    "    for dirs in Filelist3:\n",
    "        if file.split('\\\\')[4] == dirs.split('\\\\')[4]:\n",
    "            with open(file,encoding='utf-8') as f:\n",
    "                outfile = open((dirs + '\\\\'+ file.split('\\\\')[5]), 'w',encoding='utf-8')\n",
    "                lines = f.readlines()\n",
    "                for line in lines:                    \n",
    "                    line = re.sub(ptn1, '', line)\n",
    "                    line = re.sub(ptn2, '', line)\n",
    "                    line = HanziConv.toSimplified(line)\n",
    "                    outfile.write(line)\n",
    "                outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r'F:\\NLP训练\\zhwiki2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filelist(dir):\n",
    "\n",
    "    Filelist = []\n",
    "\n",
    "    for home, dirs, files in os.walk(path):\n",
    "\n",
    "        for filename in files:\n",
    "\n",
    "            # 文件名列表，包含完整路径\n",
    "\n",
    "            Filelist.append(os.path.join(home, filename))\n",
    "\n",
    "            # # 文件名列表，只包含文件名\n",
    "\n",
    "            # Filelist.append( filename)\n",
    "\n",
    "    return Filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filelist = get_filelist(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = []\n",
    "for file in  Filelist :\n",
    "    with open(file, 'r+',encoding='utf-8') as f:\n",
    "        #with open(file, 'w',encoding='utf-8') as outfile:           \n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                seg_list = list(jieba.cut(line))\n",
    "                for temp_term in seg_list:\n",
    "                    token.append(temp_term)\n",
    "            f.write(' '.join(token) + '\\n')\n",
    "            token = []\n",
    "            f.close()\n",
    "            #token.append(list(jieba.cut(lines[i]))) # database is too large, takes forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 =r'F:\\NLP训练\\zhwiki2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filelist1(dir):\n",
    "\n",
    "    Filelist = []\n",
    "\n",
    "    for home, dirs, files in os.walk(path1):\n",
    "\n",
    "        for filename in files:\n",
    "\n",
    "            # 文件名列表，包含完整路径\n",
    "\n",
    "            Filelist.append(os.path.join(home, filename))\n",
    "\n",
    "            # # 文件名列表，只包含文件名\n",
    "\n",
    "            # Filelist.append( filename)\n",
    "\n",
    "    return Filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filelist2(dir):\n",
    "\n",
    "    Filelist = []\n",
    "\n",
    "    for home, dirs, files in os.walk(path1):\n",
    "\n",
    "        for dirname in dirs:\n",
    "\n",
    "            # 文件名列表，包含完整路径\n",
    "\n",
    "            Filelist.append(os.path.join(home, dirname))\n",
    "\n",
    "            # # 文件名列表，只包含文件名\n",
    "\n",
    "            # Filelist.append( filename)\n",
    "\n",
    "    return Filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filelist2 = get_filelist2(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AA', 'AB', 'AC', 'AD', 'AE', 'AF', 'AG', 'AH', 'AI', 'AJ', 'AK', 'AL', 'AM']\n"
     ]
    }
   ],
   "source": [
    "dirss = []\n",
    "for dirname in Filelist2:\n",
    "    d = dirname.split('\\\\')[4]\n",
    "    dirss.append(d) \n",
    "print(dirss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 =r'F:\\NLP训练'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filelist3 = []\n",
    "file_name1 = path2 +str('\\\\')+ str('zhwiki3')\n",
    "os.makedirs(file_name1) #创建文件夹\n",
    "for dirs in Filelist2:\n",
    "    file_name = file_name1 + str('\\\\') +  dirs.split('\\\\')[4]    \n",
    "    Filelist3.append(file_name)\n",
    "    os.makedirs(file_name) #创建文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这页全部是对比\n",
    "\n",
    "token = []\n",
    "for file in  Filelist :\n",
    "    with open(file, 'r+',encoding='utf-8') as f:        \n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                seg_list = list(jieba.cut(line))\n",
    "                for temp_term in seg_list:\n",
    "                    token.append(temp_term)\n",
    "            f.write(' '.join(token) + '\\n')\n",
    "            token = []\n",
    "            f.close()\n",
    "            \n",
    "            \n",
    "            \n",
    "Filelist1 = get_filelist1(dir)\n",
    "token = []\n",
    "for file in  Filelist1:\n",
    "    for dirs in Filelist3:\n",
    "        if file.split('\\\\')[4] == dirs.split('\\\\')[4]:\n",
    "            with open(file,encoding='utf-8') as f:\n",
    "                outfile = open((dirs + '\\\\'+ file.split('\\\\')[5]), 'w',encoding='utf-8')\n",
    "                lines = f.readlines()                \n",
    "                for line in lines:                    \n",
    "                    #line = re.sub(ptn1, '', line)\n",
    "                    #line = re.sub(ptn2, '', line)\n",
    "                    #line = HanziConv.toSimplified(line)\n",
    "                    seg_list = list(jieba.cut(line))\n",
    "                    for temp_term in seg_list:    \n",
    "                        token.append(temp_term)\n",
    "                    #outfile.write(line)\n",
    "                #outfile.close()\n",
    "                outfile.write(' '.join(token) + '\\n')\n",
    "                token = []\n",
    "                outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 2.133 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "Filelist1 = get_filelist1(dir)\n",
    "token = []\n",
    "for file in  Filelist1:\n",
    "    for dirs in Filelist3:\n",
    "        if file.split('\\\\')[4] == dirs.split('\\\\')[4]:\n",
    "            with open(file,encoding='utf-8') as f:\n",
    "                outfile = open((dirs + '\\\\'+ file.split('\\\\')[5]), 'w',encoding='utf-8')\n",
    "                lines = f.readlines()                \n",
    "                for line in lines:                    \n",
    "                    #line = re.sub(ptn1, '', line)\n",
    "                    #line = re.sub(ptn2, '', line)\n",
    "                    #line = HanziConv.toSimplified(line)\n",
    "                    seg_list = list(jieba.cut(line))\n",
    "                    for temp_term in seg_list:    \n",
    "                        token.append(temp_term)\n",
    "                    #outfile.write(line)\n",
    "                #outfile.close()\n",
    "                outfile.write(' '.join(token) + '\\n')\n",
    "                token = []\n",
    "                outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r'F:\\NLP训练\\zhwiki3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filelist(dir):\n",
    "\n",
    "    Filelist = []\n",
    "\n",
    "    for home, dirs, files in os.walk(path):\n",
    "\n",
    "        for filename in files:\n",
    "\n",
    "            # 文件名列表，包含完整路径\n",
    "\n",
    "            Filelist.append(os.path.join(home, filename))\n",
    "\n",
    "            # # 文件名列表，只包含文件名\n",
    "\n",
    "            # Filelist.append( filename)\n",
    "\n",
    "    return Filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filelist = get_filelist(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "line23 = jieba.cut('数学是利用符号语言研究数量结构变化以及空间等概念的一门学科')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Tokenizer.cut at 0x000001ED6BE4B228>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数学/ 是/ 利用/ 符号语言/ 研究/ 数量/ 结构/ 变化/ 以及/ 空间/ 等/ 概念/ 的/ 一门/ 学科\n"
     ]
    }
   ],
   "source": [
    "print( \"/ \".join(line23))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "line24 = list(jieba.cut('数学是利用符号语言研究数量结构变化以及空间等概念的一门学科'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['数学',\n",
       " '是',\n",
       " '利用',\n",
       " '符号语言',\n",
       " '研究',\n",
       " '数量',\n",
       " '结构',\n",
       " '变化',\n",
       " '以及',\n",
       " '空间',\n",
       " '等',\n",
       " '概念',\n",
       " '的',\n",
       " '一门',\n",
       " '学科']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for term in line24:\n",
    "    l.append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['数学',\n",
       " '是',\n",
       " '利用',\n",
       " '符号语言',\n",
       " '研究',\n",
       " '数量',\n",
       " '结构',\n",
       " '变化',\n",
       " '以及',\n",
       " '空间',\n",
       " '等',\n",
       " '概念',\n",
       " '的',\n",
       " '一门',\n",
       " '学科']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数学/ 是/ 利用/ 符号语言/ 研究/ 数量/ 结构/ 变化/ 以及/ 空间/ 等/ 概念/ 的/ 一门/ 学科\n"
     ]
    }
   ],
   "source": [
    "print( \"/ \".join(line24))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "file123 = 'F:\\\\NLP训练\\\\123.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token1 = []\n",
    "with open(file123, 'r+') as f1:\n",
    "            lines1 = f1.readlines()\n",
    "            for line1 in lines1:\n",
    "                seg_list = list(jieba.cut(line1))\n",
    "                for temp_term in seg_list:\n",
    "                    token1.append(temp_term)\n",
    "            f1.write(' '.join(token1) + '\\n')\n",
    "            token1 = []\n",
    "            f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(file123, 'r+') as f1:\n",
    "            lines1 = f1.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'到了16世纪算术初等代数以及三角学等初等数学已大体完备'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token1 = []\n",
    "with open(file123, 'r+') as f1:\n",
    "            #lines1 = f1.readlines()\n",
    "            #for line1 in lines1:\n",
    "            seg_list = list(jieba.cut(line1))\n",
    "                #for temp_term in seg_list:\n",
    "                    #token1.append(temp_term)\n",
    "            f1.write(' '.join(seg_list) + '\\n')\n",
    "            #token1 = []\n",
    "            seg_list = []\n",
    "            f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token1 = []\n",
    "with open(file123, 'r+') as f1:\n",
    "            #lines1 = f1.readlines()\n",
    "            #for line1 in lines1:\n",
    "            seg_list = list(jieba.cut(file123))\n",
    "                #for temp_term in seg_list:\n",
    "                    #token1.append(temp_term)\n",
    "            f1.write(' '.join(seg_list) + '\\n')\n",
    "            #token1 = []\n",
    "            seg_list = []\n",
    "            f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
