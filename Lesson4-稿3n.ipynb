{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savefile(savepath, content):\n",
    "    fp = open(savepath, \"w\",encoding='utf-8', errors='ignore')\n",
    "    fp.write(content)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(path):\n",
    "    fp = open(path, \"r\", encoding='utf-8', errors='ignore')\n",
    "    content = fp.read()\n",
    "    fp.close()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = r'F:\\NLP训练\\zhwiki2'  # 未分词分类预料库路径\n",
    "seg_path = r'F:\\NLP训练\\zhwiki4'  # 分词后分类语料库路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "catelist = os.listdir(corpus_path)  # 获取该目录下所有子目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mydir in catelist:\n",
    "    class_path = corpus_path + str('\\\\') + mydir + \"\\\\\"  # 拼出分类子目录的路径\n",
    "    seg_dir = seg_path + str('\\\\') + mydir + \"\\\\\"  # 拼出分词后预料分类目录\n",
    "    if not os.path.exists(seg_dir):  # 是否存在，不存在则创建\n",
    "        os.makedirs(seg_dir)\n",
    "    file_list = os.listdir(class_path)\n",
    "    for file_path in file_list:\n",
    "        fullname = class_path + file_path\n",
    "        content = readfile(fullname).strip()  # 读取文件内容\n",
    "        content = content.replace(\"\\n\", \"\")  # 删除换行和多余的空格\n",
    "        content_seg = jieba.cut(content)\n",
    "        savefile(seg_dir + file_path, \" \".join(content_seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r'F:\\NLP训练\\zhwiki4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filelist(dir):\n",
    "\n",
    "    Filelist = []\n",
    "\n",
    "    for home, dirs, files in os.walk(path):\n",
    "\n",
    "        for filename in files:\n",
    "\n",
    "            # 文件名列表，包含完整路径\n",
    "\n",
    "            Filelist.append(os.path.join(home, filename))\n",
    "\n",
    "            # # 文件名列表，只包含文件名\n",
    "\n",
    "            # Filelist.append( filename)\n",
    "\n",
    "    return Filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filelist = get_filelist(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in  Filelist :\n",
    "    with open(file, 'r',encoding='utf-8') as wiki_news:\n",
    "        sentences = LineSentence(wiki_news)\n",
    "        news_model_1 = Word2Vec(sentences, min_count=5, size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_news = open('F:\\\\NLP训练\\\\zhwiki4\\\\AA\\\\wiki_00', 'r',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Word2Vec(LineSentence(wiki_news), sg=0,size=200, window=5, min_count=5, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.76084805,  1.4872608 , -0.4746232 , -0.32298478,  0.6799837 ,\n",
       "        0.13521658, -0.63590866, -0.3784451 , -0.44818148,  0.10236945,\n",
       "       -1.2141662 , -0.47412845, -0.27794865, -1.0590856 , -0.46066058,\n",
       "       -0.06817091, -0.01365431,  0.59543175,  0.5979757 , -0.88459283,\n",
       "       -0.56597674,  0.5221712 ,  0.31028378, -0.03483895, -0.9314564 ,\n",
       "       -0.5849391 , -0.7536633 , -0.64922845, -0.47214675,  1.296351  ,\n",
       "        0.9955793 ,  0.08154783, -0.97365314,  0.4727348 , -0.21370183,\n",
       "       -0.4702087 , -1.2107494 , -0.00631121, -0.6439326 ,  0.35392675,\n",
       "       -0.2002811 , -0.07652189, -0.09738079,  0.9716006 , -1.0510665 ,\n",
       "        0.17452383,  0.85657   , -0.46417442,  1.1171712 ,  0.31932405],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model_1.wv['中国']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
